{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e848ed6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T03:31:32.408275Z",
     "start_time": "2023-10-24T03:31:32.386422Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "65a92040",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T03:26:49.570129Z",
     "start_time": "2023-11-03T03:26:49.489152Z"
    }
   },
   "outputs": [],
   "source": [
    "df_house = pd.read_csv(\"kc_house_data.csv\")\n",
    "X = df['sqft_living']\n",
    "Y = df['price']\n",
    "N = len(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "365c4636",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T04:21:27.206495Z",
     "start_time": "2023-12-08T04:21:27.175961Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = df.drop(labels='price',axis=1)\n",
    "y = df['price']\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def evaluate(model,x_train, y_train, x_test, y_test):\n",
    "    model.fit(x_train,y_train)\n",
    "    \n",
    "    y_test_pred = model.predict(x_test)\n",
    "    y_train_pred = model.predict(x_train)\n",
    "    \n",
    "    Rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    R2_score_train = r2_score(y_train, y_train_pred)\n",
    "    R2_score_test = r2_score(y_test, y_test_pred)\n",
    "    return Rmse, R2_score_train, R2_score_test\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "\n",
    "scaler = ['-',MinMaxScaler(), StandardScaler(), RobustScaler()]\n",
    "\n",
    "def create_table(model):\n",
    "    n = 45\n",
    "    i = 0\n",
    "    random_state = 42\n",
    "    \n",
    "    # RESET TABLE\n",
    "    rate_table = pd.DataFrame({'Model': ['-'],\n",
    "                               'Params': ['-'],\n",
    "                               'FeatureSelection':['-'] ,\n",
    "                               'Scaler': ['-'],\n",
    "                               'Number of features': ['-'],\n",
    "                               'Rmse':['-'],\n",
    "                               'R2 Score(train)':['-'],\n",
    "                               'R2 Score(test)':['-']})\n",
    "    rate_table,i = rate_original(model,rate_table,i)\n",
    "   # rate_table,i = rate_variance_threshold(model,rate_table,i)\n",
    "    rate_table,i = rate_SKB(model,rate_table,i)\n",
    "    \n",
    "    rate_table = rate_table.iloc[1:].sort_values(by='R2 Score(test)',ascending=False)\n",
    "    display(rate_table)\n",
    "    \n",
    "    existing_df = pd.read_csv(f'exp/{model.__class__.__name__}.csv')\n",
    "    merged_df = pd.concat([existing_df, rate_table], axis=0, ignore_index=True)\n",
    "    merged_df.to_csv(f'exp/{model.__class__.__name__}.csv', index=False)\n",
    "    \n",
    "def rate_original(model,rate_table,i):\n",
    "    # Original:\n",
    "    Rmse, R2_score_train, R2_score_test = evaluate(model,X_train,y_train,X_test,y_test)\n",
    "    num_feats = X_train.shape[1]\n",
    "    row = pd.DataFrame({'Model':model.__class__.__name__,\n",
    "                        'Params': str(model.get_params()),\n",
    "                        'FeatureSelection': ['-'],\n",
    "                        'Scaler': ['-'],\n",
    "                        'Number of features': num_feats,\n",
    "                        'Rmse':Rmse,\n",
    "                        'R2 Score(train)':R2_score_train, \n",
    "                        'R2 Score(test)':R2_score_test})\n",
    "    rate_table = pd.concat([rate_table,row],axis = 0,ignore_index=True)\n",
    "    sys.stdout.write(\"\\rProgress: [{:<22}] {:.2f}%\".format(\"=\" * (i // 2), (i / n) * 100))\n",
    "    i+=1\n",
    "    sys.stdout.write(\"\\rProgress: [{:<22}] {:.2f}% \\tDone original!\".format(\"=\" * (i // 2), (i / n) * 100))\n",
    "    return rate_table,i\n",
    "\n",
    "def rate_variance_threshold(model,rate_table,i):    \n",
    "    threshold = [0.05, 0.5,1]\n",
    "    for thres in threshold:\n",
    "        selector = VarianceThreshold(threshold=thres)\n",
    "        x_train = selector.fit_transform(X_train)\n",
    "        x_test = selector.transform(X_test)\n",
    "        num_feats = x_train.shape[1]\n",
    "        for sc in scaler:\n",
    "            if sc != '-':\n",
    "                x_train = sc.fit_transform(x_train)\n",
    "                x_test = sc.transform(x_test)\n",
    "            Rmse, R2_score_train, R2_score_test = evaluate(model,x_train,y_train,x_test,y_test)\n",
    "            row = pd.DataFrame({'Model':model.__class__.__name__,\n",
    "                                'Params': str(model.get_params()),\n",
    "                                'FeatureSelection': ['VarianceThreshold'],\n",
    "                                'Scaler': sc,\n",
    "                                'Number of features': num_feats,\n",
    "                                'Rmse':Rmse,\n",
    "                                'R2 Score(train)':R2_score_train, \n",
    "                                'R2 Score(test)':R2_score_test})\n",
    "            rate_table = pd.concat([rate_table,row],axis = 0,ignore_index=True)\n",
    "            sys.stdout.write(\"\\rProgress: [{:<22}] {:.2f}%!\".format(\"=\" * (i // 2), (i / n) * 100))\n",
    "            i+=1            \n",
    "    sys.stdout.write(\"\\rProgress: [{:<22}] {:.2f}%\\tDone varThreshold!\".format(\"=\" * (i // 2), (i / n) * 100))\n",
    "    return rate_table,i\n",
    "\n",
    "def rate_SKB(model,rate_table,i):    \n",
    "    num_bests = [19,12,7]\n",
    "    for num in num_bests:\n",
    "        selection = [([\"SelectKBest\"],SelectKBest(score_func = f_classif, k=num)),\n",
    "                     #([\"RFE\"],RFE(model,n_features_to_select=num)),\n",
    "                     ([\"SFS\"],SequentialFeatureSelector(model,n_features_to_select=num,scoring='r2',cv=3))]\n",
    "        for name, selector in selection:\n",
    "            x_train = selector.fit_transform(X_train,y_train)\n",
    "            x_test = selector.transform(X_test)\n",
    "            num_feats = x_train.shape[1]\n",
    "            for sc in scaler:\n",
    "                if sc != '-':\n",
    "                    x_train = sc.fit_transform(x_train)\n",
    "                    x_test = sc.transform(x_test)\n",
    "                Rmse, R2_score_train, R2_score_test = evaluate(model,x_train,y_train,x_test,y_test)\n",
    "                row = pd.DataFrame({'Model':model.__class__.__name__,\n",
    "                                    'Params': str(model.get_params()),\n",
    "                                    'FeatureSelection': name,\n",
    "                                    'Scaler': sc,\n",
    "                                    'Number of features': num_feats,\n",
    "                                    'Rmse':Rmse,\n",
    "                                    'R2 Score(train)':R2_score_train, \n",
    "                                    'R2 Score(test)':R2_score_test})\n",
    "                rate_table = pd.concat([rate_table,row],axis = 0,ignore_index=True)\n",
    "                sys.stdout.write(\"\\rProgress: [{:<22}] {:.2f}%\".format(\"=\" * (i // 2), (i / n) * 100))\n",
    "                i+=1\n",
    "        \n",
    "        sys.stdout.write(\"\\rProgress: [{:<22}] {:.2f}%\\tDone num_feat = {}!\".format(\"=\" * (i // 2), (i / n) * 100,num))   \n",
    "        \n",
    "    return rate_table,i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7c1fdb18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T03:10:19.328130Z",
     "start_time": "2023-12-08T03:10:19.314107Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_small_table(model):\n",
    "    n = 20\n",
    "    i = 0\n",
    "    random_state = 42\n",
    "    \n",
    "    # RESET TABLE\n",
    "    rate_table = pd.DataFrame({'Model': ['-'],\n",
    "                               'Params': ['-'],\n",
    "                               'FeatureSelection':['-'] ,\n",
    "                               'Scaler': ['-'],\n",
    "                               'Number of features': ['-'],\n",
    "                               'Rmse':['-'],\n",
    "                               'R2 Score(train)':['-'],\n",
    "                               'R2 Score(test)':['-']})\n",
    "    rate_table,i = rate_original(model,rate_table,i)\n",
    "    rate_table,i = rate_SKB(model,rate_table,i)\n",
    "    \n",
    "    rate_table = rate_table.iloc[1:].sort_values(by='R2 Score(test)',ascending=False)\n",
    "    display(rate_table)\n",
    "    \n",
    "    existing_df = pd.read_csv(f'exp/{model.__class__.__name__}.csv')\n",
    "    merged_df = pd.concat([existing_df, rate_table], axis=0, ignore_index=True)\n",
    "    merged_df.to_csv(f'exp/{model.__class__.__name__}.csv', index=False)\n",
    "    \n",
    "def rate_original(model,rate_table,i):\n",
    "    # Original:\n",
    "    Rmse, R2_score_train, R2_score_test = evaluate(model,X_train,y_train,X_test,y_test)\n",
    "    num_feats = X_train.shape[1]\n",
    "    row = pd.DataFrame({'Model':model.__class__.__name__,\n",
    "                        'Params': str(model.get_params()),\n",
    "                        'FeatureSelection': ['-'],\n",
    "                        'Scaler': ['-'],\n",
    "                        'Number of features': num_feats,\n",
    "                        'Rmse':Rmse,\n",
    "                        'R2 Score(train)':R2_score_train, \n",
    "                        'R2 Score(test)':R2_score_test})\n",
    "    rate_table = pd.concat([rate_table,row],axis = 0,ignore_index=True)\n",
    "    sys.stdout.write(\"\\rProgress: [{:<33}] {:.2f}%\".format(\"=\" * (i), (i / n) * (31250000/236113)))\n",
    "    i+=2\n",
    "    sys.stdout.write(\"\\rProgress: [{:<33}] {:.2f}% \\tDone original!\".format(\"=\" * (i), (i / n) * (31250000/236113)))\n",
    "    return rate_table,i\n",
    "\n",
    "def rate_SKB(model,rate_table,i):    \n",
    "    num_bests = [19,7]\n",
    "    for num in num_bests:\n",
    "        selection = [([\"SelectKBest\"],SelectKBest(score_func = f_classif, k=num)),\n",
    "                     ([\"SFS\"],SequentialFeatureSelector(model,n_features_to_select=num,scoring='r2',cv=3))]\n",
    "        for name, selector in selection:\n",
    "            x_train = selector.fit_transform(X_train,y_train)\n",
    "            x_test = selector.transform(X_test)\n",
    "            num_feats = x_train.shape[1]\n",
    "            for sc in scaler:\n",
    "                if sc != '-':\n",
    "                    x_train = sc.fit_transform(x_train)\n",
    "                    x_test = sc.transform(x_test)\n",
    "                Rmse, R2_score_train, R2_score_test = evaluate(model,x_train,y_train,x_test,y_test)\n",
    "                row = pd.DataFrame({'Model':model.__class__.__name__,\n",
    "                                    'Params': str(model.get_params()),\n",
    "                                    'FeatureSelection': name,\n",
    "                                    'Scaler': sc,\n",
    "                                    'Number of features': num_feats,\n",
    "                                    'Rmse':Rmse,\n",
    "                                    'R2 Score(train)':R2_score_train, \n",
    "                                    'R2 Score(test)':R2_score_test})\n",
    "                rate_table = pd.concat([rate_table,row],axis = 0,ignore_index=True)\n",
    "                sys.stdout.write(\"\\rProgress: [{:<33}] {:.2f}%\".format(\"=\" * (i), (i / n) * (31250000/236113)))\n",
    "                i+=2\n",
    "        \n",
    "        sys.stdout.write(\"\\rProgress: [{:<33}] {:.2f}%\\tDone num_feat = {}!\".format(\"=\" * (i), (i / n) * (31250000/236113),num))\n",
    "        \n",
    "    return rate_table,i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219731e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T03:00:58.615763Z",
     "start_time": "2023-12-05T03:00:58.615748Z"
    }
   },
   "outputs": [],
   "source": [
    "n_estimators = [100,200]\n",
    "learning_rate = [0.01,0.3]\n",
    "max_depth = [2,5]\n",
    "num_leaves = [5,20,50]\n",
    "min_data_in_leaf = [100,500,1000]\n",
    "for n_estimator in n_estimators:\n",
    "    for eta in learning_rate:\n",
    "        for m_d in max_depth:\n",
    "            for n_l in num_leaves:\n",
    "                for m_d_i_l in min_data_in_leaf:\n",
    "                    model = lgb.LGBMRegressor(objective='regression',\n",
    "                                              n_estimators=n_estimator,\n",
    "                                              learning_rate = eta,\n",
    "                                              max_depth=m_d,\n",
    "                                              num_leaves  = n_l,\n",
    "                                              min_data_in_leaf =m_d_i_l, random_state=random_state,)\n",
    "                    create_table(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f836c9",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-06T13:42:02.454Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: 100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=\n",
      "[LightGBM] [Warning] Unknown parameter: 100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.727010 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2570\n",
      "[LightGBM] [Info] Number of data points in the train set: 8645, number of used features: 22\n",
      "[LightGBM] [Info] Start training from score 537953.549335\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Unknown parameter: 100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=\n",
      "[LightGBM] [Warning] Unknown parameter: 1000\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=\n",
      "[LightGBM] [Warning] Unknown parameter: 1000\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.275195 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2570\n",
      "[LightGBM] [Info] Number of data points in the train set: 8645, number of used features: 22\n",
      "[LightGBM] [Info] Start training from score 537582.546559\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Unknown parameter: 1000\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=\n",
      "[LightGBM] [Warning] Unknown parameter: 1000\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=\n",
      "[LightGBM] [Warning] Unknown parameter: 1000\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.074861 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2570\n",
      "[LightGBM] [Info] Number of data points in the train set: 8645, number of used features: 22\n",
      "[LightGBM] [Info] Start training from score 537953.549335\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Unknown parameter: 1000\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=\n",
      "[LightGBM] [Warning] Unknown parameter: 20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=\n",
      "[LightGBM] [Warning] Unknown parameter: 20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.390868 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] Unknown parameter: 1000\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=\n",
      "[LightGBM] [Warning] Unknown parameter: 1000\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.943791 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2570\n",
      "[LightGBM] [Info] Number of data points in the train set: 8645, number of used features: 22\n",
      "[LightGBM] [Info] Start training from score 537953.549335\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Unknown parameter: 1000\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=\n",
      "[LightGBM] [Warning] Unknown parameter: 1000\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=\n",
      "[LightGBM] [Warning] Unknown parameter: 1000\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.325743 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2570\n",
      "[LightGBM] [Info] Number of data points in the train set: 8645, number of used features: 22\n",
      "[LightGBM] [Info] Start training from score 537582.546559\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Unknown parameter: 1000\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=\n",
      "[LightGBM] [Warning] Unknown parameter: 20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=\n",
      "[LightGBM] [Warning] Unknown parameter: 20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.572125 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2570\n",
      "[LightGBM] [Info] Number of data points in the train set: 8645, number of used features: 22\n",
      "[LightGBM] [Info] Start training from score 537953.549335\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: 20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=\n",
      "[LightGBM] [Warning] Unknown parameter: 20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.776506 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2570\n",
      "[LightGBM] [Info] Number of data points in the train set: 8645, number of used features: 22\n",
      "[LightGBM] [Info] Start training from score 537582.546559\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Unknown parameter: 20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=\n",
      "[LightGBM] [Warning] Unknown parameter: 20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=\n",
      "[LightGBM] [Warning] Unknown parameter: 20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.353305 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2570\n",
      "[LightGBM] [Info] Number of data points in the train set: 8645, number of used features: 22\n",
      "[LightGBM] [Info] Start training from score 537582.546559\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: 20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=\n",
      "[LightGBM] [Warning] Unknown parameter: 20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.647495 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2570\n",
      "[LightGBM] [Info] Number of data points in the train set: 8645, number of used features: 22\n",
      "[LightGBM] [Info] Start training from score 537953.549335\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Unknown parameter: 20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=\n",
      "[LightGBM] [Warning] Unknown parameter: 20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=\n",
      "[LightGBM] [Warning] Unknown parameter: 20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.364774 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2570\n",
      "[LightGBM] [Info] Number of data points in the train set: 8645, number of used features: 22\n",
      "[LightGBM] [Info] Start training from score 537953.549335\n",
      "[LightGBM] [Warning] Unknown parameter: 20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=\n",
      "[LightGBM] [Warning] Unknown parameter: 1000\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=\n",
      "[LightGBM] [Warning] Unknown parameter: 1000\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.715692 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2570\n",
      "[LightGBM] [Info] Number of data points in the train set: 8645, number of used features: 22\n",
      "[LightGBM] [Info] Start training from score 537953.549335\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'objective': 'regression'\n",
    "}\n",
    "model = lgb.LGBMRegressor(**params)\n",
    "\n",
    "param_grid = {\n",
    "    'num_leaves': [5,50],\n",
    "    'min_data_in_leaf ': [20,100,1000],\n",
    "    'learning_rate': [0.1,0.3],\n",
    "    'n_estimators': [300,200],\n",
    "    'max_depth': [2,5]\n",
    "    }\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid,scoring = 'r2',n_jobs=20,cv=2)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "score_df = pd.DataFrame(grid_search.cv_results_)\n",
    "score_df.nlargest(10,\"mean_test_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "054bfeaf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T12:32:08.589512Z",
     "start_time": "2023-12-06T12:31:41.161227Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Progress: [                      ] 0.00%\r",
      "Progress: [                      ] 2.22% \tDone original!"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Params</th>\n",
       "      <th>FeatureSelection</th>\n",
       "      <th>Scaler</th>\n",
       "      <th>Number of features</th>\n",
       "      <th>Rmse</th>\n",
       "      <th>R2 Score(train)</th>\n",
       "      <th>R2 Score(test)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>{'objective': 'reg:squarederror', 'base_score'...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>22</td>\n",
       "      <td>136252.641064</td>\n",
       "      <td>0.94442</td>\n",
       "      <td>0.877198</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Model                                             Params  \\\n",
       "1  XGBRegressor  {'objective': 'reg:squarederror', 'base_score'...   \n",
       "\n",
       "  FeatureSelection Scaler Number of features           Rmse R2 Score(train)  \\\n",
       "1                -      -                 22  136252.641064         0.94442   \n",
       "\n",
       "  R2 Score(test)  \n",
       "1       0.877198  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = xgb.XGBRegressor(n_estimators=200, max_depth=5,alpha=100,reg_lambda=10,learning_rate=0.1, random_state=random_state)\n",
    "create_table(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "667eeea7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T04:07:36.186021Z",
     "start_time": "2023-12-08T03:25:40.072219Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [==================================] 100.00%\tDone num_feat = 7!"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Params</th>\n",
       "      <th>FeatureSelection</th>\n",
       "      <th>Scaler</th>\n",
       "      <th>Number of features</th>\n",
       "      <th>Rmse</th>\n",
       "      <th>R2 Score(train)</th>\n",
       "      <th>R2 Score(test)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VotingRegressor</td>\n",
       "      <td>{'estimators': [('xgb', XGBRegressor(alpha=100...</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>19</td>\n",
       "      <td>133221.444025</td>\n",
       "      <td>0.956238</td>\n",
       "      <td>0.882601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VotingRegressor</td>\n",
       "      <td>{'estimators': [('xgb', XGBRegressor(alpha=100...</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>19</td>\n",
       "      <td>133430.745685</td>\n",
       "      <td>0.956158</td>\n",
       "      <td>0.882232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VotingRegressor</td>\n",
       "      <td>{'estimators': [('xgb', XGBRegressor(alpha=100...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>22</td>\n",
       "      <td>133462.59885</td>\n",
       "      <td>0.960267</td>\n",
       "      <td>0.882176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>VotingRegressor</td>\n",
       "      <td>{'estimators': [('xgb', XGBRegressor(alpha=100...</td>\n",
       "      <td>SFS</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>7</td>\n",
       "      <td>133750.612677</td>\n",
       "      <td>0.938118</td>\n",
       "      <td>0.881667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>VotingRegressor</td>\n",
       "      <td>{'estimators': [('xgb', XGBRegressor(alpha=100...</td>\n",
       "      <td>SFS</td>\n",
       "      <td>-</td>\n",
       "      <td>7</td>\n",
       "      <td>133757.41184</td>\n",
       "      <td>0.938118</td>\n",
       "      <td>0.881655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VotingRegressor</td>\n",
       "      <td>{'estimators': [('xgb', XGBRegressor(alpha=100...</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>-</td>\n",
       "      <td>19</td>\n",
       "      <td>133786.693394</td>\n",
       "      <td>0.957169</td>\n",
       "      <td>0.881603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>VotingRegressor</td>\n",
       "      <td>{'estimators': [('xgb', XGBRegressor(alpha=100...</td>\n",
       "      <td>SFS</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>19</td>\n",
       "      <td>133810.755681</td>\n",
       "      <td>0.95893</td>\n",
       "      <td>0.88156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>VotingRegressor</td>\n",
       "      <td>{'estimators': [('xgb', XGBRegressor(alpha=100...</td>\n",
       "      <td>SFS</td>\n",
       "      <td>-</td>\n",
       "      <td>19</td>\n",
       "      <td>133821.847967</td>\n",
       "      <td>0.958935</td>\n",
       "      <td>0.881541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>VotingRegressor</td>\n",
       "      <td>{'estimators': [('xgb', XGBRegressor(alpha=100...</td>\n",
       "      <td>SFS</td>\n",
       "      <td>RobustScaler()</td>\n",
       "      <td>7</td>\n",
       "      <td>133902.483851</td>\n",
       "      <td>0.938113</td>\n",
       "      <td>0.881398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>VotingRegressor</td>\n",
       "      <td>{'estimators': [('xgb', XGBRegressor(alpha=100...</td>\n",
       "      <td>SFS</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>19</td>\n",
       "      <td>134033.809748</td>\n",
       "      <td>0.958979</td>\n",
       "      <td>0.881165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>VotingRegressor</td>\n",
       "      <td>{'estimators': [('xgb', XGBRegressor(alpha=100...</td>\n",
       "      <td>SFS</td>\n",
       "      <td>RobustScaler()</td>\n",
       "      <td>19</td>\n",
       "      <td>134057.075322</td>\n",
       "      <td>0.958873</td>\n",
       "      <td>0.881124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>VotingRegressor</td>\n",
       "      <td>{'estimators': [('xgb', XGBRegressor(alpha=100...</td>\n",
       "      <td>SFS</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>7</td>\n",
       "      <td>134147.765809</td>\n",
       "      <td>0.938055</td>\n",
       "      <td>0.880963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>VotingRegressor</td>\n",
       "      <td>{'estimators': [('xgb', XGBRegressor(alpha=100...</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>RobustScaler()</td>\n",
       "      <td>19</td>\n",
       "      <td>134533.205719</td>\n",
       "      <td>0.956866</td>\n",
       "      <td>0.880278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>VotingRegressor</td>\n",
       "      <td>{'estimators': [('xgb', XGBRegressor(alpha=100...</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>7</td>\n",
       "      <td>177215.510337</td>\n",
       "      <td>0.90539</td>\n",
       "      <td>0.792261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>VotingRegressor</td>\n",
       "      <td>{'estimators': [('xgb', XGBRegressor(alpha=100...</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>RobustScaler()</td>\n",
       "      <td>7</td>\n",
       "      <td>177329.035031</td>\n",
       "      <td>0.905314</td>\n",
       "      <td>0.791995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>VotingRegressor</td>\n",
       "      <td>{'estimators': [('xgb', XGBRegressor(alpha=100...</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>-</td>\n",
       "      <td>7</td>\n",
       "      <td>177360.729029</td>\n",
       "      <td>0.905566</td>\n",
       "      <td>0.79192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>VotingRegressor</td>\n",
       "      <td>{'estimators': [('xgb', XGBRegressor(alpha=100...</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>7</td>\n",
       "      <td>177361.221133</td>\n",
       "      <td>0.905567</td>\n",
       "      <td>0.791919</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Model                                             Params  \\\n",
       "3   VotingRegressor  {'estimators': [('xgb', XGBRegressor(alpha=100...   \n",
       "4   VotingRegressor  {'estimators': [('xgb', XGBRegressor(alpha=100...   \n",
       "1   VotingRegressor  {'estimators': [('xgb', XGBRegressor(alpha=100...   \n",
       "15  VotingRegressor  {'estimators': [('xgb', XGBRegressor(alpha=100...   \n",
       "14  VotingRegressor  {'estimators': [('xgb', XGBRegressor(alpha=100...   \n",
       "2   VotingRegressor  {'estimators': [('xgb', XGBRegressor(alpha=100...   \n",
       "7   VotingRegressor  {'estimators': [('xgb', XGBRegressor(alpha=100...   \n",
       "6   VotingRegressor  {'estimators': [('xgb', XGBRegressor(alpha=100...   \n",
       "17  VotingRegressor  {'estimators': [('xgb', XGBRegressor(alpha=100...   \n",
       "8   VotingRegressor  {'estimators': [('xgb', XGBRegressor(alpha=100...   \n",
       "9   VotingRegressor  {'estimators': [('xgb', XGBRegressor(alpha=100...   \n",
       "16  VotingRegressor  {'estimators': [('xgb', XGBRegressor(alpha=100...   \n",
       "5   VotingRegressor  {'estimators': [('xgb', XGBRegressor(alpha=100...   \n",
       "12  VotingRegressor  {'estimators': [('xgb', XGBRegressor(alpha=100...   \n",
       "13  VotingRegressor  {'estimators': [('xgb', XGBRegressor(alpha=100...   \n",
       "10  VotingRegressor  {'estimators': [('xgb', XGBRegressor(alpha=100...   \n",
       "11  VotingRegressor  {'estimators': [('xgb', XGBRegressor(alpha=100...   \n",
       "\n",
       "   FeatureSelection            Scaler Number of features           Rmse  \\\n",
       "3       SelectKBest    MinMaxScaler()                 19  133221.444025   \n",
       "4       SelectKBest  StandardScaler()                 19  133430.745685   \n",
       "1                 -                 -                 22   133462.59885   \n",
       "15              SFS    MinMaxScaler()                  7  133750.612677   \n",
       "14              SFS                 -                  7   133757.41184   \n",
       "2       SelectKBest                 -                 19  133786.693394   \n",
       "7               SFS    MinMaxScaler()                 19  133810.755681   \n",
       "6               SFS                 -                 19  133821.847967   \n",
       "17              SFS    RobustScaler()                  7  133902.483851   \n",
       "8               SFS  StandardScaler()                 19  134033.809748   \n",
       "9               SFS    RobustScaler()                 19  134057.075322   \n",
       "16              SFS  StandardScaler()                  7  134147.765809   \n",
       "5       SelectKBest    RobustScaler()                 19  134533.205719   \n",
       "12      SelectKBest  StandardScaler()                  7  177215.510337   \n",
       "13      SelectKBest    RobustScaler()                  7  177329.035031   \n",
       "10      SelectKBest                 -                  7  177360.729029   \n",
       "11      SelectKBest    MinMaxScaler()                  7  177361.221133   \n",
       "\n",
       "   R2 Score(train) R2 Score(test)  \n",
       "3         0.956238       0.882601  \n",
       "4         0.956158       0.882232  \n",
       "1         0.960267       0.882176  \n",
       "15        0.938118       0.881667  \n",
       "14        0.938118       0.881655  \n",
       "2         0.957169       0.881603  \n",
       "7          0.95893        0.88156  \n",
       "6         0.958935       0.881541  \n",
       "17        0.938113       0.881398  \n",
       "8         0.958979       0.881165  \n",
       "9         0.958873       0.881124  \n",
       "16        0.938055       0.880963  \n",
       "5         0.956866       0.880278  \n",
       "12         0.90539       0.792261  \n",
       "13        0.905314       0.791995  \n",
       "10        0.905566        0.79192  \n",
       "11        0.905567       0.791919  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "\n",
    "r1 = xgb.XGBRegressor(n_estimators=200, max_depth=5,alpha=100,reg_lambda=3,learning_rate=0.3, random_state=random_state)\n",
    "r2 = RandomForestRegressor(n_estimators=50,min_samples_split=20,min_samples_leaf=2,random_state=random_state)\n",
    "r3 = lgb.LGBMRegressor(objective='regression',n_estimators=1000,learning_rate = 0.01,max_depth=5,num_leaves  = 5,verbose=-1,min_data_in_leaf =100, random_state=random_state)\n",
    "\n",
    "\n",
    "model = VotingRegressor([('xgb', r1), ('rf', r2), ('L', r3)],weights=[10,1,5],n_jobs=20)\n",
    "create_small_table(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "53601e8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T03:04:20.116773Z",
     "start_time": "2023-12-08T02:26:04.991749Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [==================================] 55.56%\tDone num_feat = 7!"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Params</th>\n",
       "      <th>FeatureSelection</th>\n",
       "      <th>Scaler</th>\n",
       "      <th>Number of features</th>\n",
       "      <th>Rmse</th>\n",
       "      <th>R2 Score(train)</th>\n",
       "      <th>R2 Score(test)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>VotingRegressor</td>\n",
       "      <td>{'estimators': [('xgb', XGBRegressor(alpha=100...</td>\n",
       "      <td>SFS</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>19</td>\n",
       "      <td>126254.73154</td>\n",
       "      <td>0.966526</td>\n",
       "      <td>0.894559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>VotingRegressor</td>\n",
       "      <td>{'estimators': [('xgb', XGBRegressor(alpha=100...</td>\n",
       "      <td>SFS</td>\n",
       "      <td>-</td>\n",
       "      <td>19</td>\n",
       "      <td>126342.806314</td>\n",
       "      <td>0.966389</td>\n",
       "      <td>0.894412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>VotingRegressor</td>\n",
       "      <td>{'estimators': [('xgb', XGBRegressor(alpha=100...</td>\n",
       "      <td>SFS</td>\n",
       "      <td>RobustScaler()</td>\n",
       "      <td>19</td>\n",
       "      <td>126442.973917</td>\n",
       "      <td>0.966237</td>\n",
       "      <td>0.894244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>VotingRegressor</td>\n",
       "      <td>{'estimators': [('xgb', XGBRegressor(alpha=100...</td>\n",
       "      <td>SFS</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>19</td>\n",
       "      <td>126659.12509</td>\n",
       "      <td>0.96657</td>\n",
       "      <td>0.893882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>VotingRegressor</td>\n",
       "      <td>{'estimators': [('xgb', XGBRegressor(alpha=100...</td>\n",
       "      <td>SFS</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>7</td>\n",
       "      <td>133908.840328</td>\n",
       "      <td>0.948118</td>\n",
       "      <td>0.881387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>VotingRegressor</td>\n",
       "      <td>{'estimators': [('xgb', XGBRegressor(alpha=100...</td>\n",
       "      <td>SFS</td>\n",
       "      <td>RobustScaler()</td>\n",
       "      <td>7</td>\n",
       "      <td>133911.010235</td>\n",
       "      <td>0.948114</td>\n",
       "      <td>0.881383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>VotingRegressor</td>\n",
       "      <td>{'estimators': [('xgb', XGBRegressor(alpha=100...</td>\n",
       "      <td>SFS</td>\n",
       "      <td>-</td>\n",
       "      <td>7</td>\n",
       "      <td>133965.343849</td>\n",
       "      <td>0.948117</td>\n",
       "      <td>0.881287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>VotingRegressor</td>\n",
       "      <td>{'estimators': [('xgb', XGBRegressor(alpha=100...</td>\n",
       "      <td>SFS</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>7</td>\n",
       "      <td>134011.32082</td>\n",
       "      <td>0.94811</td>\n",
       "      <td>0.881205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VotingRegressor</td>\n",
       "      <td>{'estimators': [('xgb', XGBRegressor(alpha=100...</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>19</td>\n",
       "      <td>136729.713857</td>\n",
       "      <td>0.963823</td>\n",
       "      <td>0.876337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VotingRegressor</td>\n",
       "      <td>{'estimators': [('xgb', XGBRegressor(alpha=100...</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>19</td>\n",
       "      <td>136912.37655</td>\n",
       "      <td>0.963715</td>\n",
       "      <td>0.876006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VotingRegressor</td>\n",
       "      <td>{'estimators': [('xgb', XGBRegressor(alpha=100...</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>-</td>\n",
       "      <td>19</td>\n",
       "      <td>137166.839716</td>\n",
       "      <td>0.964526</td>\n",
       "      <td>0.875545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VotingRegressor</td>\n",
       "      <td>{'estimators': [('xgb', XGBRegressor(alpha=100...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>22</td>\n",
       "      <td>137183.012828</td>\n",
       "      <td>0.967247</td>\n",
       "      <td>0.875515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>VotingRegressor</td>\n",
       "      <td>{'estimators': [('xgb', XGBRegressor(alpha=100...</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>RobustScaler()</td>\n",
       "      <td>19</td>\n",
       "      <td>137820.402496</td>\n",
       "      <td>0.964253</td>\n",
       "      <td>0.874356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>VotingRegressor</td>\n",
       "      <td>{'estimators': [('xgb', XGBRegressor(alpha=100...</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>RobustScaler()</td>\n",
       "      <td>7</td>\n",
       "      <td>178406.473673</td>\n",
       "      <td>0.920094</td>\n",
       "      <td>0.789459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>VotingRegressor</td>\n",
       "      <td>{'estimators': [('xgb', XGBRegressor(alpha=100...</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>7</td>\n",
       "      <td>178407.807678</td>\n",
       "      <td>0.920075</td>\n",
       "      <td>0.789456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>VotingRegressor</td>\n",
       "      <td>{'estimators': [('xgb', XGBRegressor(alpha=100...</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>-</td>\n",
       "      <td>7</td>\n",
       "      <td>178444.447412</td>\n",
       "      <td>0.920084</td>\n",
       "      <td>0.78937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>VotingRegressor</td>\n",
       "      <td>{'estimators': [('xgb', XGBRegressor(alpha=100...</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>7</td>\n",
       "      <td>178449.723044</td>\n",
       "      <td>0.920089</td>\n",
       "      <td>0.789357</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Model                                             Params  \\\n",
       "8   VotingRegressor  {'estimators': [('xgb', XGBRegressor(alpha=100...   \n",
       "6   VotingRegressor  {'estimators': [('xgb', XGBRegressor(alpha=100...   \n",
       "9   VotingRegressor  {'estimators': [('xgb', XGBRegressor(alpha=100...   \n",
       "7   VotingRegressor  {'estimators': [('xgb', XGBRegressor(alpha=100...   \n",
       "15  VotingRegressor  {'estimators': [('xgb', XGBRegressor(alpha=100...   \n",
       "17  VotingRegressor  {'estimators': [('xgb', XGBRegressor(alpha=100...   \n",
       "14  VotingRegressor  {'estimators': [('xgb', XGBRegressor(alpha=100...   \n",
       "16  VotingRegressor  {'estimators': [('xgb', XGBRegressor(alpha=100...   \n",
       "3   VotingRegressor  {'estimators': [('xgb', XGBRegressor(alpha=100...   \n",
       "4   VotingRegressor  {'estimators': [('xgb', XGBRegressor(alpha=100...   \n",
       "2   VotingRegressor  {'estimators': [('xgb', XGBRegressor(alpha=100...   \n",
       "1   VotingRegressor  {'estimators': [('xgb', XGBRegressor(alpha=100...   \n",
       "5   VotingRegressor  {'estimators': [('xgb', XGBRegressor(alpha=100...   \n",
       "13  VotingRegressor  {'estimators': [('xgb', XGBRegressor(alpha=100...   \n",
       "12  VotingRegressor  {'estimators': [('xgb', XGBRegressor(alpha=100...   \n",
       "10  VotingRegressor  {'estimators': [('xgb', XGBRegressor(alpha=100...   \n",
       "11  VotingRegressor  {'estimators': [('xgb', XGBRegressor(alpha=100...   \n",
       "\n",
       "   FeatureSelection            Scaler Number of features           Rmse  \\\n",
       "8               SFS  StandardScaler()                 19   126254.73154   \n",
       "6               SFS                 -                 19  126342.806314   \n",
       "9               SFS    RobustScaler()                 19  126442.973917   \n",
       "7               SFS    MinMaxScaler()                 19   126659.12509   \n",
       "15              SFS    MinMaxScaler()                  7  133908.840328   \n",
       "17              SFS    RobustScaler()                  7  133911.010235   \n",
       "14              SFS                 -                  7  133965.343849   \n",
       "16              SFS  StandardScaler()                  7   134011.32082   \n",
       "3       SelectKBest    MinMaxScaler()                 19  136729.713857   \n",
       "4       SelectKBest  StandardScaler()                 19   136912.37655   \n",
       "2       SelectKBest                 -                 19  137166.839716   \n",
       "1                 -                 -                 22  137183.012828   \n",
       "5       SelectKBest    RobustScaler()                 19  137820.402496   \n",
       "13      SelectKBest    RobustScaler()                  7  178406.473673   \n",
       "12      SelectKBest  StandardScaler()                  7  178407.807678   \n",
       "10      SelectKBest                 -                  7  178444.447412   \n",
       "11      SelectKBest    MinMaxScaler()                  7  178449.723044   \n",
       "\n",
       "   R2 Score(train) R2 Score(test)  \n",
       "8         0.966526       0.894559  \n",
       "6         0.966389       0.894412  \n",
       "9         0.966237       0.894244  \n",
       "7          0.96657       0.893882  \n",
       "15        0.948118       0.881387  \n",
       "17        0.948114       0.881383  \n",
       "14        0.948117       0.881287  \n",
       "16         0.94811       0.881205  \n",
       "3         0.963823       0.876337  \n",
       "4         0.963715       0.876006  \n",
       "2         0.964526       0.875545  \n",
       "1         0.967247       0.875515  \n",
       "5         0.964253       0.874356  \n",
       "13        0.920094       0.789459  \n",
       "12        0.920075       0.789456  \n",
       "10        0.920084        0.78937  \n",
       "11        0.920089       0.789357  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "r1 = xgb.XGBRegressor(n_estimators=200, max_depth=5,alpha=100,reg_lambda=3,learning_rate=0.3, random_state=random_state)\n",
    "r2 = RandomForestRegressor(n_estimators=50,min_samples_split=20,min_samples_leaf=2,random_state=random_state)\n",
    "#r3 = lgb.LGBMRegressor(objective='regression',n_estimators=1000,learning_rate = 0.01,max_depth=5,num_leaves  = 5,min_data_in_leaf =100,verbose=-1, random_state=random_state)\n",
    "\n",
    "\n",
    "model = VotingRegressor([('xgb', r1), ('rf', r2)],n_jobs=20)\n",
    "create_small_table(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ca7fd7e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T04:12:21.196029Z",
     "start_time": "2023-12-08T04:12:21.192191Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "825442fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T04:17:15.747702Z",
     "start_time": "2023-12-08T04:16:25.539902Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.747756</td>\n",
       "      <td>0.099135</td>\n",
       "      <td>0.597120</td>\n",
       "      <td>0.006079</td>\n",
       "      <td>0.01</td>\n",
       "      <td>500</td>\n",
       "      <td>{'learning_rate': 0.01, 'n_estimators': 500}</td>\n",
       "      <td>0.682334</td>\n",
       "      <td>0.667695</td>\n",
       "      <td>0.675014</td>\n",
       "      <td>0.007320</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.297786</td>\n",
       "      <td>0.000529</td>\n",
       "      <td>0.012302</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.3</td>\n",
       "      <td>10</td>\n",
       "      <td>{'learning_rate': 0.3, 'n_estimators': 10}</td>\n",
       "      <td>0.659698</td>\n",
       "      <td>0.665351</td>\n",
       "      <td>0.662524</td>\n",
       "      <td>0.002826</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.332794</td>\n",
       "      <td>0.009742</td>\n",
       "      <td>0.014447</td>\n",
       "      <td>0.000268</td>\n",
       "      <td>0.01</td>\n",
       "      <td>10</td>\n",
       "      <td>{'learning_rate': 0.01, 'n_estimators': 10}</td>\n",
       "      <td>0.578617</td>\n",
       "      <td>0.634275</td>\n",
       "      <td>0.606446</td>\n",
       "      <td>0.027829</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.984669</td>\n",
       "      <td>0.046347</td>\n",
       "      <td>0.606751</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>0.3</td>\n",
       "      <td>500</td>\n",
       "      <td>{'learning_rate': 0.3, 'n_estimators': 500}</td>\n",
       "      <td>0.127852</td>\n",
       "      <td>-0.650937</td>\n",
       "      <td>-0.261543</td>\n",
       "      <td>0.389394</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "1      15.747756      0.099135         0.597120        0.006079   \n",
       "2       0.297786      0.000529         0.012302        0.000026   \n",
       "0       0.332794      0.009742         0.014447        0.000268   \n",
       "3       8.984669      0.046347         0.606751        0.006030   \n",
       "\n",
       "  param_learning_rate param_n_estimators  \\\n",
       "1                0.01                500   \n",
       "2                 0.3                 10   \n",
       "0                0.01                 10   \n",
       "3                 0.3                500   \n",
       "\n",
       "                                         params  split0_test_score  \\\n",
       "1  {'learning_rate': 0.01, 'n_estimators': 500}           0.682334   \n",
       "2    {'learning_rate': 0.3, 'n_estimators': 10}           0.659698   \n",
       "0   {'learning_rate': 0.01, 'n_estimators': 10}           0.578617   \n",
       "3   {'learning_rate': 0.3, 'n_estimators': 500}           0.127852   \n",
       "\n",
       "   split1_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "1           0.667695         0.675014        0.007320                1  \n",
       "2           0.665351         0.662524        0.002826                2  \n",
       "0           0.634275         0.606446        0.027829                3  \n",
       "3          -0.650937        -0.261543        0.389394                4  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AdaBoostRegressor(random_state=random_state)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [10,500],\n",
    "    'learning_rate': [0.01,0.3]\n",
    "    }\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid,scoring = 'r2',n_jobs=30,cv=2)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "score_df = pd.DataFrame(grid_search.cv_results_)\n",
    "score_df.nlargest(5,\"mean_test_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94329022",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-08T06:58:49.026Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [==========            ] 44.44%\tDone num_feat = 12!"
     ]
    }
   ],
   "source": [
    "model = AdaBoostRegressor(random_state=random_state,learning_rate=0.005,n_estimators=100)\n",
    "create_table(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc1f0c9",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T02:32:44.174Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "\n",
    "r1 = xgb.XGBRegressor(n_estimators=200, max_depth=5,alpha=100,reg_lambda=3,learning_rate=0.3, random_state=random_state)\n",
    "r2 = RandomForestRegressor(n_estimators=50,min_samples_split=20,min_samples_leaf=2,random_state=random_state)\n",
    "r3 = lgb.LGBMRegressor(objective='regression',n_estimators=1000,learning_rate = 0.01,max_depth=5,num_leaves  = 5,min_data_in_leaf =100, random_state=random_state,verbose=-1)\n",
    "\n",
    "\n",
    "model = VotingRegressor([('xgb', r1), ('rf', r2), ('L', r3)],weights=[5,1,1],n_jobs=20)\n",
    "create_table(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba628279",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hoang_nguyen",
   "language": "python",
   "name": "hoang_nguyen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
